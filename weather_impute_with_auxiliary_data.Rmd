---
title: "Gap filling with nearby weather stations"
author: "Quentin D. Read"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Here, I'm going to use the data from nearby weather stations that Zamir sent to try to fill in the larger gaps of the SCAN time series. The approach will be:

- Fill in any small gaps of a few hours by linear interpolation for the continuous variables (temperature, wind speed, relative humidity, radiation, and soil moisture). Do not do this for precipitation because you cannot really interpolate as it is not really a "state" of the system.
- Train random forest models to correlate the weather variables from the SCAN time series with one or more of the other locations, for the times when they are overlapping.
- Compare the models for each variable to show which locations are the best predictors for the overlapping portion, selecting the best one.
- Use the best model to predict the SCAN values for the times when we do have data from one or more nearby stations but not the SCAN time series.
- If the best model is not available for a time period, use the second best model.
- If no models are available for a time period, use an *ad hoc* imputation method based on calculating the rolling average from the five-hour period centered on the missing time point, across all years, then adding simulated noise to it. (This is needed for a period of time in 2019-2020 when we have no sensor data.)

# Fill in the shorter gaps for variables other than precipitation

Load the SCAN data and do initial processing.

```{r}
library(data.table)
library(purrr)
library(glue)
library(stringr)
library(lubridate)
library(units)
library(ggplot2)
library(caret)
library(gt)
library(Rutilitybelt)

theme_set(theme_bw() + theme(
  panel.grid = element_blank(),
  strip.background = element_blank()
))
```

```{r, eval = FALSE}
years <- 2014:2021

# Files have different number of rows to skip 
skip_n_scan <- c(2, 0, 4, 4, 4, 4, 4, 4)
skip_n_solar <- c(2, 4, 4, 4, 4, 4, 4, 4)

# Read data
scan_raw <- map2(years, skip_n_scan, ~ fread(glue('project/weather_data/2198_SCAN_WATERYEAR={.x}.csv'), skip = .y)) 
solar_raw <- map2(years, skip_n_solar, ~ fread(glue('project/weather_data/2198_SOLAR_WATERYEAR={.x}.csv'), skip = .y)) 

# Convert Date column to Date format for all, if it is not already
for (i in 1:8) {
  if (!'Date' %in% class(scan_raw[[i]][['Date']])) {
    scan_raw[[i]][, Date := as.Date(Date, format = '%m/%d/%Y')]
  } else {
    scan_raw[[i]][, Date := as.Date(Date)]
  }
  if (!'Date' %in% class(solar_raw[[i]][['Date']])) {
    solar_raw[[i]][, Date := as.Date(Date, format = '%m/%d/%Y')]
  } else {
    solar_raw[[i]][, Date := as.Date(Date)]
  }
}

scan_data <- rbindlist(scan_raw, fill = TRUE)
solar_data <- rbindlist(solar_raw, fill = TRUE)

# Get rid of bad column
scan_data[, V32 := NULL]
solar_data[, V5 := NULL]

# Get rid of bad rows that all have 23:59 as the time
scan_data <- scan_data[!Time %in% '23:59']
solar_data <- solar_data[!Time %in% '23:59']

# Some of the times are formatted differently from others. Make sure all are padded with 0 if hour has only 1 digit.
scan_data[, Time := str_pad(Time, 5, pad = '0')]
solar_data[, Time := str_pad(Time, 5, pad = '0')]
```

Join each year's data table together into one large table, create additional rows for the missing data, convert `-99.9` to `NA`, and keep only the needed variables.

```{r, eval = FALSE}
# Sort and set keys for each data table, then do a full join.
setkey(scan_data, `Site Id`, Date, Time)
setkey(solar_data, `Site Id`, Date, Time)

unique_times <- unique(rbind(scan_data[,.(`Site Id`, Date, Time)], solar_data[, .(`Site Id`, Date, Time)]))

all_raw_data <- scan_data[solar_data[unique_times]]

# Create data.frame of all the dates and times with no gaps and join the data to it, to show how many are missing.
date_range <- range(all_raw_data[['Date']])
all_date_time <- CJ(Date = seq(from = date_range[1], to = date_range[2], by = 1), Time = glue('{sprintf("%02d", 0:23)}:00'))

all_raw_data <- all_raw_data[all_date_time, on = .(Date, Time)]

# Create data frame with only the desired variables
needed_raw_data <- all_raw_data[, .(Date, Time, `TOBS.I-1 (degC)`, `WSPDX.H-1 (mph)`, `RHUM.I-1 (pct)`, `SRADV.H-1 (watt)`, `PRCP.H-1 (in)`)]

obs_vars <- c('TOBS.I-1 (degC)', 'WSPDX.H-1 (mph)', 'RHUM.I-1 (pct)', 'SRADV.H-1 (watt)', 'PRCP.H-1 (in)')

# Convert -99.9 codes to NA
needed_raw_data[, (obs_vars) := lapply(.SD, function(x) ifelse(x == -99.9, as.numeric(NA), x)), .SDcols = obs_vars]

# Add Month, Day, and Time columns
needed_raw_data[, Month := month(Date)]
needed_raw_data[, Day := day(Date)]
needed_raw_data[, Time := as.numeric(substr(Time, 1, 2))]
```

Take the rolling average over a five-hour window for the variables other than precipitation. Use these to impute the values for shorter time gaps.

```{r, eval = FALSE}
# Function with five hour moving window rolling mean 
fivehour_roll <- function(x) frollmean(x, n = 5, align = 'center', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = TRUE)

mean_vars <- obs_vars[1:4]

data_fivehour_rollmean <- copy(needed_raw_data)
data_fivehour_rollmean[, (mean_vars) := lapply(.SD, fivehour_roll), .SDcols = mean_vars]

# Fill in the missing value only if needed
data_fivehour_filled <- copy(needed_raw_data)
for (v in mean_vars) {
  data_fivehour_filled[[v]] <- ifelse(is.na(needed_raw_data[[v]]), data_fivehour_rollmean[[v]], needed_raw_data[[v]])
}
```

# Load and process the AWN and AMF weather time series

Load all the auxiliary time series. Convert the date and time stamps to the same format as the SCAN time series. Select the appropriate columns and convert them to the same units as the SCAN time series (U.S. units) -- not strictly necessary but makes it easier to interpret. Use automated unit conversion for this. Note: solar radiation in SCAN is just in unit of W so it cannot be converted to W m<sup>-2</sup> without knowing the area.

Coarsen the time resolution of the AWN and AMF time series. AWN is in 15-minute time steps and AMF in 30-minute time steps. A rolling average with resolution 4 (right aligned so we line up an hour with the measurements preceding it in time) is used to coarsen from 15 minutes to 60 minutes, and resolution 2 for 30 minutes to 60 minutes. (For precipitation, sum is used instead of mean.) Remove all time points that are not an exact multiple of an hour.

As of 19 July, we now have time series with correct time stamps for both AMF RC1 and AMF RC2.

```{r, eval = FALSE}
awn_raw <- fread('project/weather_data/DHSVM_Pullman/Weather Data/AWN_15_300152.csv')
amf_rc1_raw <- fread('project/weather_data/DHSVM_Pullman/Weather Data/AMF_US-RC1_BASE_HH_2-5.csv', skip = 2)
amf_rc2_raw <- fread('project/weather_data/DHSVM_Pullman/Weather Data/AMF_US-RC2_BASE_HH_2-5.csv', skip = 2)

units_options(set_units_mode = 'standard')
convert_units <- function(x, old, new) as.numeric(set_units(set_units(x, old), new))

### Process AWN

# AWN missing values coded as 99999
awn_raw[awn_raw == 99999] <- NA

# Convert units for temperature (and average measurements 1 and 2)
awn_raw[, air_temp_C := convert_units(apply(cbind(AIR_TEMP_F, SECOND_AIR_TEMP_F), 1, mean, na.rm = TRUE), 'degreeF', 'degreeC')]

# Get only needed variables
awn_subset <- awn_raw[, .(TSTAMP_PST, air_temp_C, `RELATIVE_HUMIDITY_%`, PRECIP_INCHES, WIND_SPEED_MPH, SOLAR_RAD_WM2)]
awn_vars <- names(awn_subset)[-1]
awn_cont_vars <- awn_vars[!awn_vars %in% 'PRECIP_INCHES']

# Get rolling mean and sum
awn_rollmean <- copy(awn_subset)
awn_rollmean[, (awn_cont_vars) := lapply(.SD, frollmean, n = 4, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = TRUE), .SDcols = awn_cont_vars]
awn_rollmean[, PRECIP_INCHES := frollsum(PRECIP_INCHES, n = 4, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = TRUE)]

awn_rollmean[, Date := as.Date(TSTAMP_PST)]
awn_rollmean[, Time := hour(TSTAMP_PST)]
awn_rollmean[, Month := month(TSTAMP_PST)]
awn_rollmean[, Day := day(TSTAMP_PST)]
awn_rollmean[, Minute := minute(TSTAMP_PST)]

awn_rollmean <- awn_rollmean[Minute == 0, .(Date, Time, Month, Day, air_temp_C, `RELATIVE_HUMIDITY_%`, PRECIP_INCHES, WIND_SPEED_MPH, SOLAR_RAD_WM2)]

### PROCESS AMF
amf_rc1_raw[, station := 'RC1']
amf_rc2_raw[, station := 'RC2']

amf_raw <- rbindlist(list(amf_rc1_raw, amf_rc2_raw))

# Missing value code is -9999
amf_raw[amf_raw == -9999] <- NA

# Convert units (wind speed m/s to mph, precip mm to in)
amf_raw[, WS := convert_units(WS, 'm/s', 'mi/h')]
amf_raw[, P := convert_units(P, 'mm', 'in')]

# Convert timestamps to dates and times. Use END date. RC2 seems to have some issues.
amf_raw[, Date := as.Date(substr(as.character(TIMESTAMP_END), 1, 8), format = '%Y%m%d')]
amf_raw[, Time := as.numeric(substr(as.character(TIMESTAMP_END), 9, 10))]
amf_raw[, Minute := as.numeric(substr(as.character(TIMESTAMP_END), 11, 12))]
amf_raw[, Month := month(Date)]
amf_raw[, Day := day(Date)]

# Get only needed variables. PPFD is used for solar radiation because it does not look like SW is in there. 
amf_subset <- amf_raw[, .(station, Date, Time, Month, Day, Minute, TA, WS, RH, PPFD_IN, P)]

amf_vars <- c('TA', 'WS', 'RH', 'PPFD_IN', 'P')
amf_cont_vars <- c('TA', 'WS', 'RH', 'PPFD_IN')

# Get rolling mean and sum. Do separately for RC1 and RC2
amf_rc1_rollmean <- amf_subset[station == 'RC1']
amf_rc1_rollmean[, (amf_cont_vars) := lapply(.SD, frollmean, n = 2, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = TRUE), .SDcols = amf_cont_vars]
amf_rc1_rollmean[, P := frollsum(P, n = 2, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = FALSE)]

amf_rc1_rollmean <- amf_rc1_rollmean[Minute == 0, .(Date, Time, Month, Day, TA, WS, RH, PPFD_IN, P)]

amf_rc2_rollmean <- amf_subset[station == 'RC2']
amf_rc2_rollmean[, (amf_cont_vars) := lapply(.SD, frollmean, n = 2, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = TRUE), .SDcols = amf_cont_vars]
amf_rc2_rollmean[, P := frollsum(P, n = 2, align = 'right', fill = NA, algo = 'exact', hasNA = TRUE, na.rm = FALSE)]

amf_rc2_rollmean <- amf_rc2_rollmean[Minute == 0, .(Date, Time, Month, Day, TA, WS, RH, PPFD_IN, P)]
```

Set the names in all the time series to be the same, as well as the order of the columns, for easier processing later. Write them to a file.

```{r, eval = FALSE}
setnames(data_fivehour_filled, c('Date', 'Time', 'T_degC', 'WS_mph', 'RH_pct', 'SR_in_Wm2', 'PRCP_in', 'Month', 'Day'))
scan_ts <- data_fivehour_filled[, .(Date, Month, Day, Time, T_degC, WS_mph, RH_pct, SR_in_Wm2, PRCP_in)]

setnames(awn_rollmean, c('Date', 'Time', 'Month', 'Day', 'T_degC', 'RH_pct', 'PRCP_in', 'WS_mph', 'SR_in_Wm2'))
awn_ts <- awn_rollmean[, .(Date, Month, Day, Time, T_degC, WS_mph, RH_pct, SR_in_Wm2, PRCP_in)]

setnames(amf_rc1_rollmean, c('Date', 'Time', 'Month', 'Day', 'T_degC', 'WS_mph', 'RH_pct', 'PPFD', 'PRCP_in'))
amfrc1_ts <- amf_rc1_rollmean[, .(Date, Month, Day, Time, T_degC, WS_mph, RH_pct, PPFD, PRCP_in)]

setnames(amf_rc2_rollmean, c('Date', 'Time', 'Month', 'Day', 'T_degC', 'WS_mph', 'RH_pct', 'PPFD', 'PRCP_in'))
amfrc2_ts <- amf_rc2_rollmean[, .(Date, Month, Day, Time, T_degC, WS_mph, RH_pct, PPFD, PRCP_in)]

save(scan_ts, awn_ts, amfrc1_ts, amfrc2_ts, file = 'project/weather_data/4ts_intermediate.RData')
```


# Side-by-side comparison of time series

Just for ease of plotting, take the daily min and max temp, the daily mean windspeed, the daily mean RH%, mean radiation, and total precipitation.

```{r}
load('project/weather_data/4ts_intermediate.RData')

daily_summ <- function(ts) {
  ts <- copy(ts)
  if ("PPFD" %in% names(ts)) setnames(ts, 'PPFD', 'SR_in_Wm2')
  ts[, .(T_min = min(T_degC), T_max = max(T_degC), WS = mean(WS_mph), RH = mean(RH_pct), SR = mean(SR_in_Wm2), P = sum(PRCP_in)), by = .(Date, Month, Day)]
}

SCAN_daily <- daily_summ(scan_ts)
AWN_daily <- daily_summ(awn_ts)
AMFRC1_daily <- daily_summ(amfrc1_ts)
AMFRC2_daily <- daily_summ(amfrc2_ts)

# Make all longform and join
date_vars <- c('Date', 'Month', 'Day')
SCAN_daily_long <- melt(SCAN_daily, id.vars = date_vars)
AWN_daily_long <- melt(AWN_daily, id.vars = date_vars)
AMFRC1_daily_long <- melt(AMFRC1_daily, id.vars = date_vars)
AMFRC2_daily_long <- melt(AMFRC2_daily, id.vars = date_vars)
SCAN_daily_long[, time_series := 'SCAN']
AWN_daily_long[, time_series := 'AWN']
AMFRC1_daily_long[, time_series := 'AMF RC1']
AMFRC2_daily_long[, time_series := 'AMF RC2']

all_daily <- rbindlist(list(SCAN_daily_long, AWN_daily_long, AMFRC1_daily_long, AMFRC2_daily_long))
```

```{r}
ggplot(all_daily, aes(x = Date, y = value, color = time_series)) +
  geom_point(size = 0.25) +
  facet_wrap(~ variable, scales = 'free_y')
```

This plot shows us that we will not be able to use the auxiliary data to impute anything in a time period from late 2019 through part of 2020 where there are no time series. We will need to use the *ad hoc* method I developed earlier for that.

Also make pairwise scatter plots for SCAN vs AMF and SCAN vs AWN. The y=x line is shown. It looks like the temperature of SCAN closely tracks AWN except that SCAN has slightly more "extreme" temperature where it tends to have lower minimum and higher maximum, but really only slightly, it has quite a bit higher average wind speed, RH and SR very similar, and P also pretty similar but maybe SCAN skews a tiny bit wetter. The exact slope of the relationship really doesn't matter as long as it is a reliable correlation which it certainly appears to be.

```{r}
all_daily_byTS <- dcast(all_daily, Date+Month+Day+variable ~ time_series)

ggplot(all_daily_byTS, aes(x = SCAN, y = AWN)) +
  geom_point(alpha = 0.25) +
  geom_abline(slope = 1, intercept = 0, color = 'red') +
  facet_wrap(~ variable, scales = 'free')
```

The Ameriflux RC1 is not as close as AWN but still pretty good. It looks like AMF has a warmer minimum temperature than SCAN, less wind, less humid, but maybe more precipitation. SR is probably the same but they are not in the same units thus why the red line does not line up. AMF RC2 is very similar to AMF RC1 but somewhat drier and windier.

```{r}
ggplot(all_daily_byTS, aes(x = SCAN, y = `AMF RC1`)) +
  geom_point(alpha = 0.25) +
  geom_abline(slope = 1, intercept = 0, color = 'red') +
  facet_wrap(~ variable, scales = 'free')
```

```{r}
ggplot(all_daily_byTS, aes(x = SCAN, y = `AMF RC2`)) +
  geom_point(alpha = 0.25) +
  geom_abline(slope = 1, intercept = 0, color = 'red') +
  facet_wrap(~ variable, scales = 'free')
```

# Predictive model

Now let's make predictive models for each weather variable to fill gaps in the time series. We will train a random forest model on the portion of the dataset where the SCAN and the other data series overlap. We will use 10-fold cross-validation approach to find the model with the best predictive power for the overlapping sections. Finally we will apply that model to fill the gaps, predicting the time points when there are no SCAN values but there are values from the other time series.

## Fit the models

*note*: This portion of the code was run on SciNet.

The random forest models are done separately for each weather variable. I think for imputation purposes it is fine to ignore covariation between the different weather variables, as well as temporal autocorrelation, as long as we can get adequate predictive power. 

I fit four random forest models for each of the five weather variables. One uses AWN only to predict, one uses AMF RC1 only, one uses AMF RC2 only, and one uses all three. Obviously more variables is better because the cross-validation will discourage overfitting, but the random forest model does not handle missing values so we can only use a smaller dataset for the model with more variables. 

For each variable, I set the prediction bounds appropriately. For example, wind speed predicted values could not be negative, and relative humidity predicted values had to be between 0 and 100.

Below is the script that was run in parallel on Ceres. This all ran in about 2 hours but was fitting many models simultaneously.

```{r, eval = FALSE}
library(data.table)
library(purrr)
library(furrr)
library(caret)
library(Rutilitybelt)

plan(multicore(workers = as.numeric(Sys.getenv('SLURM_NTASKS'))))
task <- as.numeric(Sys.getenv('SLURM_ARRAY_TASK_ID'))
pred_name <- c('AWN', 'AMFRC1', 'AMFRC2', 'all')[task]
pred_list <- list('AWN', 'AMFRC1', 'AMFRC2', c('AWN', 'AMFRC1', 'AMFRC2'))[[task]]

load('project/weather_data/4ts_intermediate.RData') # loads time series.

# Rename PPFD to SR_in_Wm2 for Ameriflux
setnames(amfrc1_ts, old = 'PPFD', new = 'SR_in_Wm2')
setnames(amfrc2_ts, old = 'PPFD', new = 'SR_in_Wm2')

# Join all data together
id_vars <- c('Date','Month','Day','Time')
scan_long <- melt(scan_ts, id.vars = id_vars, value.name = 'SCAN')
awn_long <- melt(awn_ts, id.vars = id_vars, value.name = 'AWN')
amfrc1_long <- melt(amfrc1_ts, id.vars = id_vars, value.name = 'AMFRC1')
amfrc2_long <- melt(amfrc2_ts, id.vars = id_vars, value.name = 'AMFRC2')

all_long <- Reduce(\(...) merge(..., all = TRUE), list(scan_long, awn_long, amfrc1_long, amfrc2_long))

rf_fits <- group_nest_dt(all_long, variable)

# Set prediction bounds and number of decimal places to round in post-processing for all the variables
prediction_bounds <- data.table(
  variable = c('T_degC', 'WS_mph', 'RH_pct', 'PRCP_in', 'SR_in_Wm2'),
  lower_bound = c(NA, 0, 0, 0, 0),
  upper_bound = c(NA, NA, 100, NA, NA),
  n_decimal = c(1, 1, 0, 2, 2)
)

rf_fits <- rf_fits[prediction_bounds, on = 'variable']

# function to fit random forest model for a particular dataset (variable) and predictors
fit_rf <- function(variable, data, lower_bound, upper_bound, predictors, ...) {
  data_fit <- data[, mget(c('SCAN', predictors))]
  data_fit <- data_fit[complete.cases(data_fit)]
  
  rf_control <- trainControl(method = 'cv', number = 10, 
                             allowParallel = TRUE, verboseIter = TRUE, 
                             predictionBounds = c(lower_bound, upper_bound))
  
  train(
    form = as.formula(paste('SCAN ~', paste(predictors, collapse = '+'))),
    data = data_fit,
    method = 'rf',
    trControl = rf_control
  )
}

# Apply the function with the predictor(s) for this array task
rf_fits[, fit := future_pmap(rf_fits, fit_rf, predictors = pred_list)]
rf_fits[, predict_insample := map(fit, predict)]
rf_fits[, predict_outsample := map2(fit, data, ~ predict(.x, newdata = .y))]

# Additional post-processing can be done locally later.
saveRDS(rf_fits, file = paste0('project/weather_data/rf_fits', pred_name, '.rds'))
```

## Post-processing of output

Next, I did some post-processing, also on Ceres, to combine the model fit statistics into a table and the predicted values into a large data frame, that I downloaded to work with locally.

```{r, eval = FALSE}
library(data.table)
library(caret)
library(purrr)
library(glue)

pred_names <- c('AWN', 'AMFRC1', 'AMFRC2', 'all')
pred_list <- list('AWN', 'AMFRC1', 'AMFRC2', c('AWN', 'AMFRC1', 'AMFRC2'))

fits <- map(pred_names, ~ readRDS(glue('project/weather_data/rf_fits{.}.rds'))) # Load objects

# Get summary information on final fit quality
fit_summaries <- cbind(
  expand.grid(variable = fits[[1]][['variable']], predictors = pred_names),
  map_dfr(fits, \(fitdt) map_dfr(fitdt[['fit']], \(fit) fit[['results']][fit[['results']][['mtry']] == fit[['bestTune']][1,1], ]))
)

# Create data frame of complete cases that were used for out of sample prediction in each case
walk(1:4, \(i) {
  vars <- pred_list[[i]]
  data_subset <- map(fits[[i]][['data']], \(dat) dat[complete.cases(dat[, ..vars])])
  fits[[i]][, data_subset := data_subset]
})

# Combine all predicted values together for each fit type, rounding appropriately
make_pred_data <- function(variable, n_decimal, data_subset, predict_outsample, ...) {
  y_pred <- round(predict_outsample, n_decimal)
  cbind(data.table(variable = variable, y_pred = y_pred), data_subset)
}

all_pred_list <- map(fits, \(fit) pmap_dfr(fit, make_pred_data))

# Put everything into single data.table and write to CSV
all_pred_dt <- map2_dfr(pred_names, all_pred_list, ~ data.table(predictors = .x, .y))
all_pred_dt[, c('SCAN', 'AWN', 'AMFRC1', 'AMFRC2') := NULL]

fwrite(all_pred_dt, 'project/weather_data/rf_all_pred.csv')
fwrite(fit_summaries, 'project/weather_data/rf_fit_summaries.csv')
```

## Look at fit statistics

Here are some fit statistics. The root mean squared error (RMSE) for each variable is in the data units. For example, the random forest model using only AWN as a predictor was wrong, on average, by 1.63 degrees C, for the out-of-sample predictions of each cross-validation fold. The R-squared is another measure of the model fit. It is the cross-validation R-squared so it appropriately accounts for overfitting.

The table tells us that the best prediction accuracy (highest cross-validated R-squared and lowest cross-validated RMSE) was achieved using either the AWN time series alone (for temperature and humidity), or a combination of all three (for wind speed, solar radiation, and precipitation). The accuracy for precipitation and wind speed is not as good as for the other variables but the R-squared values are well over 0.8 in both cases so I think that is good enough.

Unfortunately, though, the Ameriflux data do not exist for some of the data gaps. So we will have to use the model based on AWN for those gaps.

```{r, echo = FALSE}
fit_summaries <- fread('project/weather_data/rf_fit_summaries.csv')

gt(fit_summaries[, .(variable, predictors, RMSE, Rsquared)], groupname_col = 'variable') %>%
  fmt_number(c(RMSE, Rsquared), n_sigfig = 3) %>%
  tab_options(
    row_group.background.color = "#D4EBF2",
    row_group.font.weight = 'bold',
    column_labels.font.weight = 'bold'
  )
```

# Combine raw and imputed data

Now let's put together the raw data with the data imputed from the random forest model.

We have to do some "wrangling" to fill the data gaps with the predicted values from the random forest model. Use the AWN model for temperature and humidity for all time gaps, the all-predictors model for wind speed, radiation, and precipitation for the older time gap, and the AWN model for all for the newer time gaps where the all-predictors model does not have predicted values. Also there are some small gaps in the all-predictors model for wind speed for 2015 and 2016 which we can fill with the AWN model.

This gets us to a pretty good place where we have everything filled in up to the gap from 2019-2020 where there are no auxiliary time series. 

```{r, eval = FALSE}
load('project/weather_data/4ts_intermediate.RData') # loads time series.
rf_all_pred <- fread('project/weather_data/rf_all_pred.csv')

# Get the predictions for the AWN and all time series and make it a wide form.
rf_pred_wide <- dcast(rf_all_pred[predictors %in% c('AWN', 'all')],
                      Date + Month + Day + Time + variable ~ predictors, value.var = 'y_pred')

# SCAN time series to long form
id_vars <- c('Date','Month','Day','Time')
scan_filled <- melt(scan_ts, id.vars = id_vars, value.name = 'SCAN')

# Join observed and predicted data
scan_filled[rf_pred_wide, on = .NATURAL, `:=` (SCAN_pred_AWN = i.AWN, SCAN_pred_all = i.all)]
            
# Fill gap with the logic described above
scan_filled[variable %in% c('T_degC', 'RH_pct') | Date > '2016-09-30', SCAN_filled := ifelse(!is.na(SCAN), SCAN, SCAN_pred_AWN)]
scan_filled[variable %in% c('WS_mph', 'PRCP_in', 'SR_in_Wm2') & Date <= '2016-09-30', SCAN_filled := ifelse(!is.na(SCAN), SCAN, ifelse(!is.na(SCAN_pred_all), SCAN_pred_all, SCAN_pred_AWN))]
```

# Fill in the remaining gaps with ad hoc method

This is what we need to do to fill in the 2019-2020 gap.

First take the doubly-rolling long-run average over all years within +/- 2 days from each day, using either the observed data for that day-hour combination or the values imputed by the "short gap" filling method if applicable.

```{r,eval = FALSE}
get_fiveday_mean <- function(Date, Time, ...) {
  days_use <- Date + -2:2
  m_d_t_use <- data.table(Month = month(days_use), Day = day(days_use), Time = Time)
  data_use <- scan_ts[m_d_t_use, on = .(Month, Day, Time)]
  data_use[, lapply(.SD, mean, na.rm = TRUE), .SDcols = mean_vars]
}

mean_vars <- c('T_degC', 'WS_mph', 'RH_pct', 'SR_in_Wm2')
fiveday_means <- pmap(scan_ts, get_fiveday_mean) |> rbindlist()
```

## Add noise to variables other than precipitation

Now add noise to the imputed data using my ad hoc procedure. The general procedure is to exploit the identity that the variance of the sum of two uncorrelated normally distributed variables is the sum of their two variances. Thus we can generate Gaussian noise  $N \sim \textbf{Normal}(0, \sigma_{N})$ with $\sigma_{N} = \sqrt{\sigma^2_{observed} - \sigma^2_{imputed}}$, then add it to the imputed data, and it will have the same variance as the observed data. For temperature, this works as is. For relative humidity which ranges from 0% to 100%, we have to divide the observed data by 100 then logit-transform, to get it to approximate a normal distribution that you can add normal noise to. Add the noise then back-transform. I thought a similar approach would work with a log transformation of wind speed (it has a lower bound of 0 and appears to be roughly lognormally distributed) but I think the formula for noise standard deviation is invalid in this case so I manually tweaked the noise until it gave passable results. I did not modify shortwave radiation with noise, because it already seemed to have similar variability to the observed data without adding any more noise.

```{r, eval = FALSE}
fiveday_means_withnoise <- copy(fiveday_means)

fiveday_means_withnoise[, WS_mph := log(WS_mph)]
fiveday_means_withnoise[, RH_pct := qlogis(RH_pct/100)]

dat_observed <- scan_ts[, ..mean_vars]

dat_observed[, WS_mph := log(WS_mph)]
dat_observed[, RH_pct := qlogis(RH_pct/100)]

var_imputed <- sapply(fiveday_means_withnoise, var, na.rm = TRUE)
var_observed <- sapply(dat_observed, \(x) var(x[is.finite(x)], na.rm = TRUE))
sd_noise <- sqrt(var_observed - var_imputed)

# Apply a ballpark standard deviation to the wind speed variable, calculated by hand
sd_noise[2] <- 0.4

set.seed(410)
noise <- sapply(sd_noise, \(sd) rnorm(n = nrow(fiveday_means), mean = 0, sd = sd))
# No noise added to shortwave radiation: set to zero
noise[, 'SR_in_Wm2'] <- 0

# Add noise and back-transform, setting any remaining negative values to zero
fiveday_means_withnoise <- fiveday_means_withnoise + noise
fiveday_means_withnoise[, WS_mph := exp(WS_mph)]
fiveday_means_withnoise[, RH_pct := plogis(RH_pct) * 100]

# Round the imputed data to the same number of decimal places as the original data
fiveday_means_withnoise[, T_degC := round(T_degC, 1)]
fiveday_means_withnoise[, WS_mph := round(WS_mph, 1)]
fiveday_means_withnoise[, RH_pct := round(RH_pct, 0)]
fiveday_means_withnoise[, SR_in_Wm2 := round(SR_in_Wm2, 2)]
```


## Sample values from appropriate times for precipitation

For precipitation, regardless of the length of the missing run, we will randomly sample an hourly value from +/- 2 hours and +/- 2 days in the current year and all other years.

```{r, eval = FALSE}
random_precip_value <- function(Date, Time, ...) {
  days_use <- Date + -2:2
  hr <- as.numeric(substr(Time, 1, 2))
  times_use <- (hr + -2:2) %% 24
  m_d_t_use <- data.table(Month = month(days_use), Day = day(days_use))
  m_d_t_use <- m_d_t_use[, .(Time = times_use), by = .(Month, Day)]
  data_use <- scan_ts[m_d_t_use, on = .(Month, Day, Time)]
  sample(na.omit(data_use[["PRCP_in"]]), size = 1)
}

set.seed(919)
fiveday_precip <- pmap_dbl(scan_ts, random_precip_value)
```

## Fill in any remaining gaps

Fill in all remaining missing values with the imputed values.

```{r, eval = FALSE}
fiveday_means_combined <- cbind(scan_ts[, .(Date, Month, Day, Time)], fiveday_means_withnoise, PRCP_in = fiveday_precip)
fiveday_means_combined_long <- melt(fiveday_means_combined, id.vars = id_vars, value.name = 'SCAN_imputed_adhoc')

scan_filled[fiveday_means_combined_long, on = .NATURAL, SCAN_imputed_adhoc := i.SCAN_imputed_adhoc]

scan_filled[is.na(SCAN_filled), SCAN_filled := SCAN_imputed_adhoc]
```

Make a flag column showing what kind of imputation was done.

```{r, eval = FALSE}
scan_filled[, imputed := fcase(
  !is.na(SCAN), 'none',
  is.na(SCAN) & (!is.na(SCAN_pred_AWN) | !is.na(SCAN_pred_all)), 'nearby stations',
  is.na(SCAN) & is.na(SCAN_pred_AWN) & is.na(SCAN_pred_all), 'ad hoc method'
)]
```

Save the result.

```{r, eval = FALSE}
fwrite(scan_filled, 'project/weather_data/weather_data_all_imputed.csv')
```


# Plot the results

Plot the time series colored by the source of the data. Green is the actual data from the SCAN time series (not imputed or imputed with linear interpolation, not shown as a separate color), blue is data imputed with nearby weather stations (good quality imputation), and red is data imputed with my *ad hoc* method where no nearby weather stations were available (probably poor quality imputation).

In my opinion it all passes the eyeball test.

```{r, echo = FALSE}
dat <- fread('project/weather_data/weather_data_all_imputed.csv')
dat[, Time := str_pad(paste0(Time, ':00'), width = 5, pad = '0')]
dat[, datetime := as_datetime(paste(Date, Time), format = "%Y-%m-%d %H:%M", tz = "America/Chicago")]


wateryear_shading <- data.frame(d_min = as_datetime(c('2013-10-01 00:01', '2015-10-01 00:01', '2017-10-01 00:01', '2019-10-01 00:01'), format = "%Y-%m-%d %H:%M", tz = "America/Chicago"),
                                d_max = as_datetime(c('2014-09-30 23:59', '2016-09-30 23:59', '2018-09-30 23:59', '2020-09-30 23:59'), format = "%Y-%m-%d %H:%M", tz = "America/Chicago"))

ggplot(dat[variable == 'T_degC']) +
  geom_rect(data = wateryear_shading, aes(xmin = d_min, xmax = d_max, ymin = -Inf, ymax = Inf), fill = 'gray75') +
  geom_point(aes(x = datetime, y = SCAN_filled, color = imputed), size = 0.5, alpha = 0.5) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = 'none') +
  scale_color_brewer(palette = 'Set1') + 
  ggtitle('Temperature')

ggplot(dat[variable == 'WS_mph']) +
  geom_rect(data = wateryear_shading, aes(xmin = d_min, xmax = d_max, ymin = -Inf, ymax = Inf), fill = 'gray75') +
  geom_point(aes(x = datetime, y = SCAN_filled, color = imputed), size = 0.5, alpha = 0.5) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = 'none') +
  scale_color_brewer(palette = 'Set1') + 
  ggtitle('Wind speed')

ggplot(dat[variable == 'RH_pct']) +
  geom_rect(data = wateryear_shading, aes(xmin = d_min, xmax = d_max, ymin = -Inf, ymax = Inf), fill = 'gray75') +
  geom_point(aes(x = datetime, y = SCAN_filled, color = imputed), size = 0.5, alpha = 0.5) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = 'none') +
  scale_color_brewer(palette = 'Set1') + 
  ggtitle('Relative humidity')

ggplot(dat[variable == 'SR_in_Wm2']) +
  geom_rect(data = wateryear_shading, aes(xmin = d_min, xmax = d_max, ymin = -Inf, ymax = Inf), fill = 'gray75') +
  geom_point(aes(x = datetime, y = SCAN_filled, color = imputed), size = 0.5, alpha = 0.5) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = 'none') +
  scale_color_brewer(palette = 'Set1') + 
  ggtitle('Incoming shortwave radiation')

ggplot(dat[variable == 'PRCP_in']) +
  geom_rect(data = wateryear_shading, aes(xmin = d_min, xmax = d_max, ymin = -Inf, ymax = Inf), fill = 'gray75') +
  geom_point(aes(x = datetime, y = SCAN_filled, color = imputed), size = 0.5, alpha = 0.5) +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position = 'none') +
  scale_color_brewer(palette = 'Set1') + 
  ggtitle('Precipitation')
```


# Format the results for the DHSVM model

Reshape the filled data to long form.

```{r, eval = FALSE}
scan_filled_wide <- dcast(scan_filled, Date + Time ~ variable, value.var = 'SCAN_filled')
```


Calculate the longwave radiation column using the Stefan-Boltzmann law, `j = emissivity * sigma * (temp_C + 273.15)^4`, where `emissivity` is set at `0.9` (unitless), and `sigma` is the Stefan-Boltzmann constant `5.670374e-8 W m-2 K-4`.

```{r, eval = FALSE}
sigma <- 5.670374e-8
emissivity <- 0.9
scan_filled_wide[, `SR_out_Wm2` := round(emissivity * sigma * (T_degC + 273.15)^4, 2)]
```

Also write file in the DHSVM format. Order columns as required by the format, and remove headers.  Convert windspeed units from mi/h to m/s, and precip units from in/h to m/h.

```{r, eval = FALSE}
datetime_formatted <- with(scan_filled_wide, paste(format(Date, '%m/%d/%Y'), Time, sep = '-'))
cols_order <- c("T_degC", "WS_mph", "RH_pct", "SR_in_Wm2", "SR_out_Wm2", "PRCP_in")
dhsvm_data <- data.table(DateTime = datetime_formatted, scan_filled_wide[, ..cols_order])

dhsvm_data[, PRCP_in := PRCP_in * 0.0254]
dhsvm_data[, WS_mph := WS_mph * 0.44704]

fwrite(dhsvm_data, 'project/weather_data/dhsvm_forcing_imputed.txt', sep = ' ', quote = FALSE, col.names = FALSE)
```
